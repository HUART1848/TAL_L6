{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://heig-vd.ch/docs/default-source/doc-global-newsletter/2020-slim.svg\" alt=\"HEIG-VD Logo\" width=\"100\"/>\n",
    "\n",
    "# Cours TAL - Laboratoire 6\n",
    "# Trois méthodes de désambiguïsation lexicale\n",
    "\n",
    "**Objectif**\n",
    "\n",
    "L'objectif de ce laboratoire est d'implémenter et de comparer plusieurs méthodes de désambiguïsation lexicale (en anglais, *Word Sense Disambiguation* ou WSD).  Vous utiliserez un corpus avec plusieurs milliers de phrases, chaque phrase contenant une occurrence du mot anglais *interest* annotée avec le sens que ce mot possède dans la phrase respective.  Les trois méthodes sont les suivantes (elles seront détaillées par la suite) :\n",
    "\n",
    "* Algorithme de Lesk simplifié.\n",
    "* Utilisation de word2vec.\n",
    "* Classification supervisée utilisant des traits lexicaux :\n",
    "   - les mots en position -1, -2, ..., et +1, +2, ..., par rapport à *interest* ;\n",
    "   - apparition de mots indicateurs dans le voisinage de *interest*.\n",
    "\n",
    "Les deux premières méthodes n'utilisent pas l'apprentissage automatique.  Elles fonctionnent selon le même principe : comparer le contexte d'une occurrence de *interest* avec chacune des définitions des sens (*synsets*) et choisir la définition la plus proche du contexte.  L'algorithme de Lesk définit la proximité comme le nombre de mots en commun, alors que word2vec la calcule comme la similarité de vecteurs.  La dernière méthode vise à classifier les occurrences de *interest*, les sens étant les classes, et les attributs étant les mots du contexte (apprentissage supervisé)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Analyse des données\n",
    "\n",
    "Téléchargez le corpus *interest* depuis le [site du Prof. Ted Pedersen](http://www.d.umn.edu/~tpederse/data.html) (il se trouve en bas de la page).  Téléchargez l'archive ZIP marquée *original format without POS tags* et extrayez le fichier `interest-original.txt`.  Téléchargez également le fichier `README.int.txt` indiqué à la ligne au-dessus. Veuillez répondre brièvement aux questions suivantes :\n",
    "\n",
    "a. Quelles sont les URL du fichier ZIP et celle du fichier `README.int.txt` ?\n",
    "\n",
    "b. Quel est le format du fichier `interest-original.txt` et comment sont annotés les sens de *interest* ?\n",
    "\n",
    "c. Est-ce qu'on considère aussi les occurrences au pluriel (*interests*) ?\n",
    "\n",
    "d. Comment sont annotées les phrases qui contiennent plusieurs occurrences du mot *interest* ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez répondre ici (en commentaire) aux questions.\n",
    "# A. \n",
    "# https://www.d.umn.edu/~tpederse/Data/interest-original.nopos.tar.gz et https://www.d.umn.edu/~tpederse/Data/README.int.txt\n",
    "\n",
    "# B.\n",
    "# Il contient 2368 phrases contenant au moins une occurence du mot `interest`,\n",
    "# séparées ligne par ligne par le symbole '$$'\n",
    "\n",
    "# C. \n",
    "# Oui\n",
    "\n",
    "# D.\n",
    "# L'occurence supplémentaire est annotée comme `*interest`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1e.** D'après le fichier `README.int.txt`, quelles sont les définitions des six sens de *interest* annotés dans les données et quelles sont leurs fréquences ? Vous pouvez copier/coller l'extrait de `README`ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez répondre ici (en commentaire) à la question.\n",
    "# Les occurences du mot `interest` sont  annotées de 1 à 6 en fonction du sens comme suit:\n",
    "# Sense 1 =  361 occurrences (15%) - readiness to give attention\n",
    "# Sense 2 =   11 occurrences (01%) - quality of causing attention to be given to\n",
    "# Sense 3 =   66 occurrences (03%) - activity, etc. that one gives attention to\n",
    "# Sense 4 =  178 occurrences (08%) - advantage, advancement or favor\n",
    "# Sense 5 =  500 occurrences (21%) - a share in a company or business\n",
    "# Sense 6 = 1252 occurrences (53%) - money paid for the use of money"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1f.** De quel dictionnaire viennent les sens précédents ? Où peut-on le consulter en ligne ?  Veuillez aligner les définitions du dictionnaire avec les six sens annotés en écrivant par exemple `Sense 3 = \"an activity that you enjoy doing or a subject that you enjoy studying\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez répondre ici (en commentaire) à la question.\n",
    "# Le 'Longman Dictionary of Contemporary English'\n",
    "# Sense 1 = if you have an interest in something or someone, you want to know or learn more about them\n",
    "# Sense 2 = a quality or feature of something that attracts your attention or makes you want to know more about it\n",
    "# Sense 3 = an activity that you enjoy doing or a subject that you enjoy studying\n",
    "# Sense 4 = the things that bring advantages to someone or something\n",
    "# Sense 5 = if you have an interest in a particular company or industry, you own shares in it\n",
    "# Sense 6 = the extra money that you must pay back when you borrow money"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1g.** En consultant [WordNet en ligne](http://wordnetweb.princeton.edu/perl/webwn), trouvez les définitions des synsets  pour le **nom commun** *interest*.  Combien de synsets y a-t-il ?  Veuillez indiquer comme avant la **définition** de chaque synset pour chacun des six sens ci-dessus (au besoin, fusionner ou ignorer des synsets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez répondre ici (en commentaire) à la question.\n",
    "# WordNet contient les 7 synsets suivant:\n",
    "# Synsets 1 = a sense of concern with and curiosity about someone or something\n",
    "# Synsets 2 = a reason for wanting something done\n",
    "# Synsets 3 = the power of attracting or holding one's attention (because it is unusual or exciting etc.)\n",
    "# Synsets 4 = a fixed charge for borrowing money; usually a percentage of the amount borrowed\n",
    "# Synsets 5 = (law) a right or legal share of something; a financial involvement with something\n",
    "# Synsets 6 = (usually plural) a social group whose members control some field of activity and who have common aims\n",
    "# Synsets 7 = a diversion that occupies one's time and thoughts (usually pleasantly)\n",
    "\n",
    "# On peut établir la correspondance suivante:\n",
    "# Sense 1 = Synset 1\n",
    "# Sense 2 = Synset 3\n",
    "# Sense 3 = Synset 7\n",
    "# Sense 4 = Synset 2\n",
    "# Sense 5 = Synset 5\n",
    "# Sense 6 = Synset 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1h.** Définissez (manuellement, ou avec quelques lignes de code) une liste nommée `senses1` avec les mots des définitions du README, en supprimant les stopwords (p.ex. les mots < 4 lettres).  Affichez la liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['give', 'readiness', 'attention'], ['causing', 'attention', 'given', 'quality'], ['that', 'etc.', 'activity,', 'attention', 'gives'], ['advantage,', 'advancement', 'favor'], ['company', 'business', 'share'], ['paid', 'money']]\n"
     ]
    }
   ],
   "source": [
    "# Veuillez répondre ici à la question et créer la variable 'senses1' (liste de 6 listes de chaînes)\n",
    "readme_senses = [\n",
    "    \"readiness to give attention\",\n",
    "    \"quality of causing attention to be given to\",\n",
    "    \"activity, etc. that one gives attention to\",\n",
    "    \"advantage, advancement or favor\",\n",
    "    \"a share in a company or business\",\n",
    "    \"money paid for the use of money\"\n",
    "]\n",
    "\n",
    "senses1 = [list(set(filter(lambda s: len(s) >= 4, s.split()))) for s in readme_senses]\n",
    "\n",
    "print(senses1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1i.** En combinant les définitions obtenues aux points (4) et (5) ci-dessus, construisez une liste nommée `senses2` avec pour chacun des sens de *interest* une liste de **mots-clés** correspondants.  Vous pouvez concaténer les définitions, puis écrire des instructions en Python pour extraire les mots (uniques).  Respectez l'ordre des sens données par `README`, et à la fin affichez `senses2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['something', 'someone,', 'learn', 'more', 'them', 'readiness', 'attention', 'interest', 'know', 'have', 'about', 'give', 'want'], ['that', 'feature', 'causing', 'given', 'quality', 'your', 'more', 'attention', 'know', 'attracts', 'something', 'about', 'makes', 'want'], ['that', 'etc.', 'activity,', 'activity', 'studying', 'doing', 'attention', 'subject', 'enjoy', 'gives'], ['favor', 'that', 'bring', 'something', 'advantage,', 'things', 'someone', 'advantages', 'advancement'], ['particular', 'industry,', 'company', 'shares', 'interest', 'share', 'have', 'business'], ['when', 'that', 'back', 'borrow', 'must', 'paid', 'money', 'extra']]\n"
     ]
    }
   ],
   "source": [
    "# Veuillez répondre ici à la question et créer la variable 'senses2' (liste de 6 listes de chaînes).\n",
    "ldoce_senses = [\n",
    "    \"if you have an interest in something or someone, you want to know or learn more about them\",\n",
    "    \"a quality or feature of something that attracts your attention or makes you want to know more about it\",\n",
    "    \"an activity that you enjoy doing or a subject that you enjoy studying\",\n",
    "    \"the things that bring advantages to someone or something\",\n",
    "    \"if you have an interest in a particular company or industry, you own shares in it\",\n",
    "    \"the extra money that you must pay back when you borrow money\"\n",
    "]\n",
    "\n",
    "senses2 = [list(set(filter(lambda s: len(s) >= 4, s.split()))) for s in map(lambda t: t[0] + \" \" + t[1], zip(readme_senses, ldoce_senses))]\n",
    "\n",
    "print(senses2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1j.** Chargez les données depuis `interest-original.txt` dans une liste appelée `sentences` qui contient pour chaque phrase la liste des mots (sans les séparateurs *$$* et *===...*).  Ces phrases sont-elles déjà tokenisées en mots ?  Sinon, faites-le.  À ce stade, ne modifiez pas encore les occurrences annotées *interest(s)\\_X*.  Comptez le nombre total de phrases et affichez-en trois au hasard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_interest(filename):\n",
    "    TO_IGNORE = [\"$\", \"=\", \".\", \"'\", \",\", \"\\n\", \"`\"]\n",
    "    ret = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            for t in TO_IGNORE:\n",
    "                line = line.replace(t, \"\")\n",
    "            if line != \"\":\n",
    "                ret.append(nltk.tokenize.word_tokenize(line))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 2368 phrases.\n",
      "En voici 3 au hasard :\n",
      "[['investor', 'interest_1', 'in', 'stock', 'funds', 'has', 'nt', 'stalled', 'at', 'all', 'mr', 'hines', 'maintains'], ['it', 'is', 'in', 'the', 'western', 'interest_4', 'to', 'see', 'mr', 'gorbachev', 'succeed'], ['revco', 'insists', 'that', 'the', 'proposal', 'is', 'simply', 'an', 'expression', 'of', 'interest_1', 'because', 'under', 'chapter', '11', 'revco', 'has', 'exclusivity', 'rights', 'until', 'feb', '28']]\n"
     ]
    }
   ],
   "source": [
    "# Veuillez répondre ici à la question.\n",
    "sentences = []\n",
    "\n",
    "sentences = load_interest(\"interest-original.txt\")\n",
    "\n",
    "print(\"Il y a {} phrases.\\nEn voici 3 au hasard :\".format(len(sentences)))\n",
    "print(sentences[151:154])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithme de Lesk simplifié\n",
    "\n",
    "**2a.** Définissez une fonction `wsd_lesk(senses, sentence)` qui prend deux arguments : une liste de listes de mots-clés (comme `senses1` et `senses2` ci-dessus) et une phrase avec une occurrence annotée de *interest* ou *interests*, et qui retourne l'index du sens le plus probable (entre 1 et 6) selon l'algorithme de Lesk.  Cet algorithme choisit le sens qui a le maximum de mots en commun avec le contexte de *interest*.  Vous pouvez choisir vous-mêmes la taille de ce voisinage (`window_size`).  En cas d'égalité entre deux sens, tirer la réponse au sort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez répondre ici à la question.\n",
    "def wsd_lesk(senses, sentence, window_size=2):\n",
    "    pos = [i for i, w in enumerate(sentence) if re.match(f\"^interest[s]?_[1-{len(senses)}]$\", w)][0]\n",
    "    begin = max(0, pos - window_size)\n",
    "    end = min(len(sentence) - 1, pos + window_size)\n",
    "\n",
    "    window_words = sentence[begin:pos] + sentence[pos+1:end+1]\n",
    "    occ = [len([w for w in window_words if w in sense]) for sense in senses]\n",
    "\n",
    "    best = max(occ)\n",
    "    bests = [i for i, o in enumerate(occ) if o == best]\n",
    "    return random.choice(bests) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2b.** Définissez maintenant une fonction `evaluate_wsd(fct_name, senses, sentences)` qui prend en paramètre le nom de la méthode de similarité (pour commencer : `wsd_lesk`) ainsi que la liste des mots-clés par sens, et la liste de phrases, et qui retourne le score de la méthode de similarité.  Ce score sera tout simplement le pourcentage de réponses correctes (sens trouvé identique au sens annoté)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez répondre ici à la question.\n",
    "def evaluate_wsd(fct_name, senses, sentences, window_size=2):\n",
    "    n_correct = 0\n",
    "    for sentence in sentences:\n",
    "        true = int([w for w in sentence if re.match(f\"^interest[s]?_[1-{len(senses)}]$\", w)][0][-1])\n",
    "        pred = fct_name(senses, sentence, window_size)\n",
    "\n",
    "        if true == pred:\n",
    "            n_correct += 1\n",
    "\n",
    "    return n_correct / len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2c.** En fixant au mieux la taille de la fenêtre autour de *interest*, quel est le meilleur score de la méthode de Lesk simplifiée ?  Quelle liste de sens conduit à de meilleurs scores, `senses1` ou `senses2` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 2 senses1: 0.166 senses2: 0.176\n",
      "window: 3 senses1: 0.174 senses2: 0.192\n",
      "window: 4 senses1: 0.194 senses2: 0.190\n",
      "window: 5 senses1: 0.191 senses2: 0.206\n",
      "window: 6 senses1: 0.184 senses2: 0.211\n",
      "window: 7 senses1: 0.183 senses2: 0.212\n",
      "window: 8 senses1: 0.176 senses2: 0.232\n",
      "window: 9 senses1: 0.185 senses2: 0.217\n",
      "window: 10 senses1: 0.188 senses2: 0.209\n",
      "window: 11 senses1: 0.187 senses2: 0.216\n",
      "window: 12 senses1: 0.184 senses2: 0.212\n",
      "window: 13 senses1: 0.184 senses2: 0.228\n",
      "window: 14 senses1: 0.181 senses2: 0.225\n",
      "window: 15 senses1: 0.172 senses2: 0.232\n",
      "window: 16 senses1: 0.183 senses2: 0.227\n",
      "window: 17 senses1: 0.186 senses2: 0.218\n",
      "window: 18 senses1: 0.184 senses2: 0.232\n",
      "window: 19 senses1: 0.195 senses2: 0.229\n",
      "window: 20 senses1: 0.182 senses2: 0.237\n",
      "window: 21 senses1: 0.183 senses2: 0.248\n",
      "window: 22 senses1: 0.189 senses2: 0.231\n",
      "window: 23 senses1: 0.174 senses2: 0.240\n",
      "window: 24 senses1: 0.177 senses2: 0.234\n",
      "window: 25 senses1: 0.185 senses2: 0.233\n",
      "window: 26 senses1: 0.174 senses2: 0.229\n",
      "window: 27 senses1: 0.184 senses2: 0.231\n",
      "window: 28 senses1: 0.182 senses2: 0.240\n",
      "window: 29 senses1: 0.188 senses2: 0.238\n",
      "window: 30 senses1: 0.176 senses2: 0.229\n",
      "best senses1 at 19: 0.19467905405405406\n",
      "best senses2 at 21: 0.24788851351351351\n"
     ]
    }
   ],
   "source": [
    "# Veuillez répondre ici à la question.\n",
    "best_a = (0, 0)\n",
    "best_b = (0, 0)\n",
    "for window_size in range(2, 30+1):\n",
    "    a = evaluate_wsd(wsd_lesk, senses1, sentences, window_size=window_size)\n",
    "    b = evaluate_wsd(wsd_lesk, senses2, sentences, window_size=window_size)\n",
    "\n",
    "    if a > best_a[0]:\n",
    "        best_a = a, window_size\n",
    "\n",
    "    if b > best_b[0]:\n",
    "        best_b = b, window_size\n",
    "\n",
    "    print(f\"window: {window_size} senses1: {a:.3f} senses2: {b:.3f}\")\n",
    "\n",
    "print(f\"best senses1 at {best_a[1]}: {best_a[0]}\")\n",
    "print(f\"best senses2 at {best_b[1]}: {best_b[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le meilleur résultat est ~24%, obtenu avec la liste `senses2` et un `window_size` de 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utilisation de word2vec pour la similarité contexte vs. synset\n",
    "\n",
    "**3a.** En réutilisant une partie du code de `wsd_lesk`, veuillez maintenant définir une fonction `wsd_word2vec(senses, sentence)` qui choisit le sens en utilisant la similarité **word2vec** étudiée dans le labo précédent. \n",
    "* Vous pouvez chercher dans la [documentation des KeyedVectors](https://radimrehurek.com/gensim/models/keyedvectors.html) comment calculer directement la similarité entre deux listes de mots.\n",
    "* Comme `wsd_lesk`, la nouvelle fonction `wsd_word2vec` prend en argument une liste de listes de mots-clés par sens (comme `senses1` et `senses2` ci-dessus), et une phrase avec une occurrence annotée de *interest* ou *interests*.\n",
    "* La fonction retourne le numéro du sens le plus probable selon la similarité word2vec entre les mots du sens et ceux du voisinage de *interest*.  En cas d'égalité, tirer le sens au sort.\n",
    "* Vous pouvez régler la taille du voisinage (`window_size`) par l'expérimentation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gensim.downloader as api\n",
    "#api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "path_to_model = \"/home/hugo/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\" # à adapter\n",
    "wv_model = gensim.models.KeyedVectors.load_word2vec_format(path_to_model, binary=True)  # C bin format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez répondre ici à la question.\n",
    "def wsd_word2vec(senses, sentence, window_size=2):\n",
    "    pos = [i for i, w in enumerate(sentence) if re.match(f\"^interest[s]?_[1-{len(senses)}]$\", w)][0]\n",
    "    begin = max(0, pos - window_size)\n",
    "    end = min(len(sentence) - 1, pos + window_size)\n",
    "\n",
    "    window_words = sentence[begin:pos] + sentence[pos+1:end+1]\n",
    "    occ = [wv_model.n_similarity(window_words, sense) for sense in senses]\n",
    "\n",
    "    best = max(occ)\n",
    "    bests = [i for i, o in enumerate(occ) if o == best]\n",
    "    return random.choice(bests) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3b.** Appliquez maintenant la même méthode `evaluate_wsd` avec la fonction `wsd_word2vec` (en cherchant une bonne valeur de la taille de la fenêtre) et affichez le score de la similarité word2vec.  Comment se compare-t-il avec le score précédent (Lesk) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 2 senses1: 0.277 senses2: 0.419\n",
      "window: 4 senses1: 0.305 senses2: 0.423\n",
      "window: 6 senses1: 0.320 senses2: 0.435\n",
      "window: 8 senses1: 0.307 senses2: 0.433\n",
      "window: 10 senses1: 0.320 senses2: 0.436\n",
      "window: 12 senses1: 0.315 senses2: 0.440\n",
      "window: 14 senses1: 0.311 senses2: 0.437\n",
      "window: 16 senses1: 0.307 senses2: 0.436\n",
      "window: 18 senses1: 0.308 senses2: 0.438\n",
      "window: 20 senses1: 0.308 senses2: 0.438\n",
      "window: 22 senses1: 0.306 senses2: 0.435\n",
      "window: 24 senses1: 0.304 senses2: 0.434\n",
      "window: 26 senses1: 0.305 senses2: 0.432\n",
      "window: 28 senses1: 0.304 senses2: 0.432\n",
      "window: 30 senses1: 0.305 senses2: 0.434\n",
      "best senses1 at 6: 0.32010135135135137\n",
      "best senses2 at 12: 0.4396114864864865\n"
     ]
    }
   ],
   "source": [
    "# Veuillez répondre ici à la question.\n",
    "best_a = (0, 0)\n",
    "best_b = (0, 0)\n",
    "for window_size in range(2, 30+1, 2):\n",
    "    a = evaluate_wsd(wsd_word2vec, senses1, sentences, window_size=window_size)\n",
    "    b = evaluate_wsd(wsd_word2vec, senses2, sentences, window_size=window_size)\n",
    "\n",
    "    if a > best_a[0]:\n",
    "        best_a = a, window_size\n",
    "\n",
    "    if b > best_b[0]:\n",
    "        best_b = b, window_size\n",
    "\n",
    "    print(f\"window: {window_size} senses1: {a:.3f} senses2: {b:.3f}\")\n",
    "\n",
    "print(f\"best senses1 at {best_a[1]}: {best_a[0]}\")\n",
    "print(f\"best senses2 at {best_b[1]}: {best_b[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On obtient le meilleur score de ~44% avec `sense2` et une `window_size` de 12, soit un meilleur\n",
    "# résultat qu'avec l'algorithme de Lesk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification supervisée avec des traits lexicaux\n",
    "Vous entraînerez maintenant des classifieurs pour prédire le sens d'une occurrence dans une phrase.  Le principal défi sera de transformer chaque phrase en un ensemble de traits (attributs, *features*), pour créer les données en vue des expériences de classification.\n",
    "\n",
    "Veuillez utiliser le classifieur `NaiveBayesClassifier` fourni par NLTK.  Le mode d'emploi se trouve dans le [Chapitre 6, sections 1.1-1.3](https://www.nltk.org/book/ch06.html) du livre NLTK.  Consultez-le attentivement pour trouver comment formater les données.  De plus, il faudra séparer les données en sous-ensembles d'entraînement et de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Attributs lexicaux positionnels\n",
    "\n",
    "Dans cette première représentation des attributs, vous les coderez comme `mot-2`, `mot-1`, `mot+1`, `mot+2`, etc. (fenêtre de taille `2*window_size` autour de *interest*) et vous leur donnerez les valeurs des mots observés aux emplacements respectifs, ou `NONE` si la fenêtre dépasse l'étendue de la phrase.  Vous ajouterez un attribut qui est le mot *interest* lui-même, qui peut être au singulier ou au pluriel.  Pour chaque occurrence de *interest*, vous devrez donc créer une représentation formelle, incluant un dictionnaire Python et le numéro du sens :\n",
    "```\n",
    "[{'word-1': 'in', 'word+1': 'rates', 'word-2': 'declines', 'word+2': 'NONE', 'word0': 'interest'}, 6]\n",
    "```\n",
    "Vous regrouperez toutes ces entrées dans une liste totale appelée `items_with_features_A`.  (Le numéro du sens servira à l'entraînement, puis il sera caché à l'évaluation, quand on comparera la prédiction du système au numéro correct.)  \n",
    "\n",
    "**4.1a.** En partant de la liste des phrases appelée `sentences`(préparée plus haut), veuillez générer ici cette liste totale, en vous aidant si nécessaire du livre NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featuresA(sentences, n_senses=6, window_size=2):\n",
    "    ret = []\n",
    "    for sentence in sentences:\n",
    "        pos = [i for i, w in enumerate(sentence) if re.match(f\"^interest[s]?_[1-{n_senses}]$\", w)][0]\n",
    "\n",
    "        cur = {}\n",
    "        for i in range(-window_size, window_size+1):\n",
    "            match pos + i:\n",
    "                case x if x < 0: cur[f\"word{i}\"] = \"NONE\"\n",
    "                case x if x >= len(sentence): cur[f\"word+{i}\"] = \"NONE\"\n",
    "                case x if i < 0: cur[f\"word{i}\"] = sentence[x]\n",
    "                case x if i > 0: cur[f\"word+{i}\"] = sentence[x]\n",
    "                case _: cur[\"word0\"] = sentence[x][:-2]\n",
    "        ret.append([cur, int(sentence[pos][-1])])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2368\n",
      "[[{'word-3': 'NONE', 'word-2': 'NONE', 'word-1': 'investor', 'word0': 'interest', 'word+1': 'in', 'word+2': 'stock', 'word+3': 'funds'}, 1], [{'word-3': 'in', 'word-2': 'the', 'word-1': 'western', 'word0': 'interest', 'word+1': 'to', 'word+2': 'see', 'word+3': 'mr'}, 4], [{'word-3': 'an', 'word-2': 'expression', 'word-1': 'of', 'word0': 'interest', 'word+1': 'because', 'word+2': 'under', 'word+3': 'chapter'}, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Veuillez répondre ici à la question.\n",
    "items_with_features_A = featuresA(sentences, window_size=3)\n",
    "\n",
    "print(len(items_with_features_A))\n",
    "print(items_with_features_A[151:154])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1b.** On souhaite maintenant entraîner un classifieur sur une partie des données, et le tester sur une autre.  Vous garderez 80% des données pour l'entraînement et utiliserez les 20% restants pour l'évaluation.  Veuillez faire cette division séparément pour chaque sens, pour que les deux ensembles contiennent les mêmes proportions de sens que l'ensemble de départ (\"stratification\"), et enregistrer les deux sous-ensembles de `items_with_features_A` sous les noms respectifs de `iwf_A_train` et `iwf_A_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(features, n_senses=6, ratio=0.8):\n",
    "    train = []\n",
    "    test  = []\n",
    "    # Veuillez répondre ici à la question.\n",
    "\n",
    "    features = features.copy()\n",
    "    shuffle(features)\n",
    "    for sense in range(1, len(senses1) + 1):\n",
    "        sense_items = [items for items in features if items[1] == sense]\n",
    "\n",
    "        cutoff = int(round(len(sense_items) * ratio))\n",
    "        train += sense_items[:cutoff]\n",
    "        test += sense_items[cutoff:]\n",
    "        \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1895   473\n",
      "[[{'word-3': 'watching', 'word-2': 'them', 'word-1': 'with', 'word0': 'interest', 'word+1': 'NONE', 'word+2': 'NONE', 'word+3': 'NONE'}, 1], [{'word-3': 'research', 'word-2': 'institute', 'word-1': 'said', 'word0': 'interest', 'word+1': 'in', 'word+2': 'exploration', 'word+3': 'is'}, 1]] [[{'word-3': '700', 'word-2': 'million', 'word-1': 'in', 'word0': 'interest', 'word+1': 'payments', 'word+2': 'a', 'word+3': 'year'}, 6], [{'word-3': 'NONE', 'word-2': 'earnings', 'word-1': 'before', 'word0': 'interest', 'word+1': 'and', 'word+2': 'tax', 'word+3': 'from'}, 6]]\n"
     ]
    }
   ],
   "source": [
    "iwf_A_train, iwf_A_test = split_train_test(items_with_features_A, len(senses1))\n",
    "\n",
    "print(len(iwf_A_train), ' ', len(iwf_A_test))\n",
    "print(iwf_A_test[:2], iwf_A_test[-2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1c.** Veuillez créer une instance de `NaiveBayesClassifier`, l'entraîner sur `iwf_A_train` et la tester sur `iwf_A_test` (voir la documentation NLTK).  En expérimentant avec différentes largeurs de fenêtres, quel est le meilleur score global que vous obtenez (avec la fonction `accuracy`), et comment se compare-t-il avec les précédents ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window: 1 score: 0.858\n",
      "window: 3 score: 0.860\n",
      "window: 5 score: 0.860\n",
      "window: 7 score: 0.856\n",
      "window: 9 score: 0.854\n",
      "window: 11 score: 0.839\n",
      "window: 13 score: 0.812\n",
      "window: 15 score: 0.793\n",
      "window: 17 score: 0.825\n",
      "window: 19 score: 0.827\n",
      "window: 21 score: 0.791\n",
      "window: 23 score: 0.770\n",
      "window: 25 score: 0.784\n",
      "window: 27 score: 0.772\n",
      "window: 29 score: 0.751\n",
      "best at 3: 0.8604651162790697\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import naivebayes \n",
    "# Veuillez répondre ici à la question.\n",
    "\n",
    "best = (0, 0)\n",
    "for window_size in range(1, 30+1, 2):\n",
    "    items_with_features = featuresA(sentences, window_size=window_size)\n",
    "    train, test = split_train_test(items_with_features)\n",
    "\n",
    "    clf = naivebayes.NaiveBayesClassifier.train(train)\n",
    "    score = nltk.classify.accuracy(clf, test)\n",
    "\n",
    "    if score > best[1]:\n",
    "        best = (window_size, score)\n",
    "    print(f\"window: {window_size} score: {score:.3f}\")\n",
    "\n",
    "print(f\"best at {best[0]}: {best[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On obtient le meilleur score de loin jusqu'à maintenant, avec ~86% pour une `window_size`\n",
    "# ayant une faible influence à partir de ~3 et qui empire le résultat à partir d'une valeur d'environ 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.864693446088795\n"
     ]
    }
   ],
   "source": [
    "clf = naivebayes.NaiveBayesClassifier.train(iwf_A_train)\n",
    "print(nltk.classify.accuracy(clf, iwf_A_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1d.** En utilisant la fonction `show_most_informative_features()`, veuillez afficher les attributs les plus informatifs et commenter le résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  word+1 = 'in'                1 : 6      =     72.4 : 1.0\n",
      "                   word0 = 'interests'         3 : 1      =     66.7 : 1.0\n",
      "                  word-1 = 'other'             3 : 6      =     35.0 : 1.0\n",
      "                  word+1 = 'of'                4 : 6      =     32.1 : 1.0\n",
      "                  word-2 = 'have'              1 : 6      =     23.9 : 1.0\n",
      "                  word-3 = 'NONE'              6 : 3      =     22.1 : 1.0\n",
      "                  word+2 = 'the'               1 : 3      =     21.3 : 1.0\n",
      "                  word+2 = 'and'               6 : 5      =     21.2 : 1.0\n",
      "                  word+2 = 'on'                6 : 5      =     20.8 : 1.0\n",
      "                  word-1 = 'in'                6 : 5      =     17.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "clf.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On constate que les attributs les plus informatifs sont plutôt ceux qui sont les plus proches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Présence de mots indicateurs\n",
    "\n",
    "Une deuxième façon d'encoder les attributs lexicaux est de constituer un vocabulaire avec tous les mots qui apparaissent dans tous les voisinages de *interest* et de définir ces mots comme attributs.  Par conséquent, pour chaque occurrence de *interest*, on extrait la valeur de ces attributs sous la forme :\n",
    "```\n",
    "[{('rate' : True), ('in' : False), ...}, 1]\n",
    "```\n",
    "où *'rate'*, *'in'*, etc., sont les mots du vocabulaire, True/False indiquent leur présence/absence autour de l'occurrence de *interest* qui nous intéresse, et le dernier nombre est le sens, entre 1 et 6.\n",
    "\n",
    "**4.2a.** Pour commencer, en partant de `sentences` et en fixant la taille de la fenêtre, veuillez constituer la liste de tous les mots observés autour de tous les voisinages de toutes les occurrences de *interest*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_list(sentences, n_senses=6, window_size=2):\n",
    "    ret = []\n",
    "    for sentence in sentences:\n",
    "        pos = [i for i, w in enumerate(sentence) if re.match(f\"^interest[s]?_[1-{n_senses}]$\", w)][0]\n",
    "        begin = max(0, pos - window_size)\n",
    "        end = min(len(sentence) - 1, pos + window_size)\n",
    "\n",
    "        window_words = sentence[begin:pos] + sentence[pos+1:end+1]\n",
    "        ret += window_words\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13040\n",
      "['further', 'declines', 'in', 'rates', 'to', 'indicate', 'declining', 'rates', 'because', 'they', 'rises', 'in', 'short-term', 'rates', 'a', '834', '%', 'in', 'this', 'energy-services', 'holding', 'company', 'with', 'in', 'the', 'mechanical', 'be', 'refunded', 'plus', 'curry', 'set', 'the', 'rate', 'on', 'the', 'country', 's', 'own', 'prompted', 'the', 'improvements', 'of', 'principal', 'and', 'is', 'the', 'only', 'to', 'increase', 'its']\n"
     ]
    }
   ],
   "source": [
    "word_list = get_word_list(sentences, window_size=3)\n",
    "# Veuillez répondre ici à la question.\n",
    "\n",
    "print(len(word_list))\n",
    "print(word_list[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2b.** En utilisant par exemple un objet de type `nltk.FreqDist`, veuillez sélectioner les 500 mots les plus fréquents (vous pourrez aussi optimiser ce nombre), dans une liste appelée `vocabulary`.  À votre avis, est-ce une bonne idée d'enlever les *stopwords* de cette liste pour construire les traits ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voc_from_word_list(word_list, voc_size=500):\n",
    "    return list(map(lambda kv: kv[0], sorted(dict(nltk.FreqDist(word_list)).items(), key=lambda kv: kv[1], reverse=True)[:voc_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'the', 'rates', 'and', 'to', 'of', 'a', 'on', 'rate', 's', 'its', '%', 'payments', 'that', 'are', 'has', 'lower', 'with', 'for', 'an', 'is', 'by', 'have', 'will', 'high', 'at', 'short', 'from', 'company', 'or', 'annual', 'their', 'us', 'higher', 'foreign', 'which', 'as', 'minority', 'said', 'bonds', 'other', 'it', 'income', 'nt', 'short-term', 'but', 'pay', 'because', 'be', 'below']\n"
     ]
    }
   ],
   "source": [
    "# Veuillez répondre ici à la question.\n",
    "vocabulary = voc_from_word_list(word_list)\n",
    "print(vocabulary[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non, ce n'est pas forcément une bonne idée car ils peuvent aider à déterminer l'usage du mot\n",
    "# `interest` selon le sens. p. ex. `it could be *of* interest *to*`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2c.** Veuillez maintenant créer l'ensemble total de données formatées, en convertissant chaque phrase contenant une occurrence de *interest* à un dictionnaire de traits/valeurs (suivi du numéro du sens), comme exemplifié au début de cette section 4.2.  Cet ensemble sera appelé `items_with_features_B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featuresB(sentences, vocabulary, n_senses=6):\n",
    "    ret = []\n",
    "    for sentence in sentences:\n",
    "        pos = [i for i, w in enumerate(sentence) if re.match(f\"^interest[s]?_[1-{n_senses}]$\", w)][0]\n",
    "        cur = {}\n",
    "        for w in vocabulary:\n",
    "            cur[w] = w in sentence\n",
    "\n",
    "        ret.append([cur, int(sentence[pos][-1])])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2368\n"
     ]
    }
   ],
   "source": [
    "items_with_features_B = featuresB(sentences, vocabulary)\n",
    "print(len(items_with_features_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2d.** Comme dans la section 4.1, veuillez créer maintenant deux sous-ensembles de `items_with_features_B` appelés `iwf_B_train` (80% des items) et `iwf_B_test` (20% des items), avec une sélection aléatoire mais stratifiée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1895   473\n"
     ]
    }
   ],
   "source": [
    "iwf_B_train, iwf_B_test = split_train_test(items_with_features_B)\n",
    "\n",
    "print(len(iwf_B_train), ' ', len(iwf_B_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2e.** Comme dans la section 4.1, veuillez créer une instance de `NaiveBayesClassifier`, l'entraîner sur `iwf_B_train` et la tester sur `iwf_B_test`.  En expérimentant avec différentes largeurs de fenêtres et tailles du vocabulaire, quel est le meilleur score que vous obtenez, et comment se compare-t-il avec les précédents ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voc: 100: window: 1 score: 0.742\n",
      "voc: 100: window: 3 score: 0.757\n",
      "voc: 100: window: 5 score: 0.744\n",
      "voc: 100: window: 7 score: 0.742\n",
      "voc: 100: window: 9 score: 0.721\n",
      "voc: 100: window: 11 score: 0.691\n",
      "voc: 100: window: 13 score: 0.738\n",
      "voc: 100: window: 15 score: 0.753\n",
      "voc: 100: window: 17 score: 0.744\n",
      "voc: 100: window: 19 score: 0.742\n",
      "voc: 100: window: 21 score: 0.732\n",
      "voc: 100: window: 23 score: 0.746\n",
      "voc: 100: window: 25 score: 0.784\n",
      "voc: 100: window: 27 score: 0.736\n",
      "voc: 100: window: 29 score: 0.786\n",
      "voc: 500: window: 1 score: 0.812\n",
      "voc: 500: window: 3 score: 0.814\n",
      "voc: 500: window: 5 score: 0.839\n",
      "voc: 500: window: 7 score: 0.801\n",
      "voc: 500: window: 9 score: 0.795\n",
      "voc: 500: window: 11 score: 0.818\n",
      "voc: 500: window: 13 score: 0.841\n",
      "voc: 500: window: 15 score: 0.833\n",
      "voc: 500: window: 17 score: 0.814\n",
      "voc: 500: window: 19 score: 0.822\n",
      "voc: 500: window: 21 score: 0.812\n",
      "voc: 500: window: 23 score: 0.822\n",
      "voc: 500: window: 25 score: 0.833\n",
      "voc: 500: window: 27 score: 0.805\n",
      "voc: 500: window: 29 score: 0.846\n",
      "voc: 1000: window: 1 score: 0.816\n",
      "voc: 1000: window: 3 score: 0.827\n",
      "voc: 1000: window: 5 score: 0.852\n",
      "voc: 1000: window: 7 score: 0.850\n",
      "voc: 1000: window: 9 score: 0.858\n",
      "voc: 1000: window: 11 score: 0.827\n",
      "voc: 1000: window: 13 score: 0.854\n",
      "voc: 1000: window: 15 score: 0.814\n",
      "voc: 1000: window: 17 score: 0.831\n",
      "voc: 1000: window: 19 score: 0.850\n",
      "voc: 1000: window: 21 score: 0.825\n",
      "voc: 1000: window: 23 score: 0.846\n",
      "voc: 1000: window: 25 score: 0.844\n",
      "voc: 1000: window: 27 score: 0.827\n",
      "voc: 1000: window: 29 score: 0.846\n",
      "best at voc: 1000 window: 9: 0.8583509513742071\n"
     ]
    }
   ],
   "source": [
    "best = (0, 0, 0)\n",
    "for voc_size in (100, 500, 1000):\n",
    "    vocabulary = voc_from_word_list(get_word_list(sentences, window_size=window_size), voc_size=voc_size)\n",
    "    for window_size in range(1, 30+1, 2):\n",
    "        items_with_features = featuresB(sentences, vocabulary)\n",
    "        train, test = split_train_test(items_with_features)\n",
    "\n",
    "        clf = naivebayes.NaiveBayesClassifier.train(train)\n",
    "        score = nltk.classify.accuracy(clf, test)\n",
    "\n",
    "        if score > best[2]:\n",
    "            best = (voc_size, window_size, score)\n",
    "        print(f\"voc: {voc_size}: window: {window_size} score: {score:.3f}\")\n",
    "\n",
    "print(f\"best at voc: {best[0]} window: {best[1]}: {best[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8625792811839323\n"
     ]
    }
   ],
   "source": [
    "vocabulary = voc_from_word_list(get_word_list(sentences, window_size=17), voc_size=1000)\n",
    "items_with_features = featuresB(sentences, vocabulary)\n",
    "train, test = split_train_test(items_with_features)\n",
    "\n",
    "clf = naivebayes.NaiveBayesClassifier.train(train)\n",
    "score = nltk.classify.accuracy(clf, test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2f.** Quels sont les attributs les plus informatifs ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('broadcasting', True),\n",
       " ('resigned', True),\n",
       " ('area', True),\n",
       " ('computer', True),\n",
       " ('washington', True),\n",
       " ('rates', True),\n",
       " ('pursue', True),\n",
       " ('best', True),\n",
       " ('personal', True),\n",
       " ('oil', True),\n",
       " ('m', True),\n",
       " ('p', True),\n",
       " ('special', True),\n",
       " ('session', True),\n",
       " ('court', True),\n",
       " ('end', True),\n",
       " ('especially', True),\n",
       " ('find', True),\n",
       " ('law', True),\n",
       " ('little', True),\n",
       " ('next', True),\n",
       " ('priced', True),\n",
       " ('series', True),\n",
       " ('set', True),\n",
       " ('we', True),\n",
       " ('rate', True),\n",
       " ('given', True),\n",
       " ('leaving', True),\n",
       " ('our', True),\n",
       " ('investor', True),\n",
       " ('maker', True),\n",
       " ('values', True),\n",
       " ('products', True),\n",
       " ('airline', True),\n",
       " ('protecting', True),\n",
       " ('contract', True),\n",
       " ('director', True),\n",
       " ('down', True),\n",
       " ('five', True),\n",
       " ('probably', True),\n",
       " ('officer', True),\n",
       " ('expressed', True),\n",
       " ('almost', True),\n",
       " ('home', True),\n",
       " ('political', True),\n",
       " ('lower', True),\n",
       " ('public', True),\n",
       " ('might', True),\n",
       " ('francs', True),\n",
       " ('customers', True),\n",
       " ('protect', True),\n",
       " ('could', True),\n",
       " ('likely', True),\n",
       " ('says', True),\n",
       " ('week', True),\n",
       " ('president', True),\n",
       " ('open', True),\n",
       " ('short', True),\n",
       " ('news', True),\n",
       " ('ford', True),\n",
       " ('because', True),\n",
       " ('energy', True),\n",
       " ('acquire', True),\n",
       " ('going', True),\n",
       " ('like', True),\n",
       " ('national', True),\n",
       " ('official', True),\n",
       " ('conflict', True),\n",
       " ('great', True),\n",
       " ('takeover', True),\n",
       " ('property', True),\n",
       " ('building', True),\n",
       " ('columbia', True),\n",
       " ('him', True),\n",
       " ('important', True),\n",
       " ('main', True),\n",
       " ('together', True),\n",
       " ('higher', True),\n",
       " ('both', True),\n",
       " ('business', True),\n",
       " ('include', True),\n",
       " ('bought', True),\n",
       " ('cellular', True),\n",
       " ('properties', True),\n",
       " ('payments', True),\n",
       " ('into', True),\n",
       " ('all', True),\n",
       " ('institutional', True),\n",
       " ('who', True),\n",
       " ('debt', True),\n",
       " ('%', True),\n",
       " ('taken', True),\n",
       " ('bonds', True),\n",
       " ('own', True),\n",
       " ('controlling', True),\n",
       " ('holds', True),\n",
       " ('long', True),\n",
       " ('ltd', True),\n",
       " ('rather', True),\n",
       " ('chief', True)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "Veuillez recopier ci-dessous, en guise de conclusion, les scores des quatre expériences réalisées, pour pouvoir les comparer d'un coup d'oeil.  Quel est le meilleur score obtenu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lesk: ~24%\n",
    "# Word2Vec: ~44%\n",
    "# Attributs positionnels: ~86%\n",
    "# Vocabulaire: ~86%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin du laboratoire\n",
    "\n",
    "Merci de nettoyer votre feuille, sauvegarder le résultat, et soumettre le *notebook* sur Cyberlearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
